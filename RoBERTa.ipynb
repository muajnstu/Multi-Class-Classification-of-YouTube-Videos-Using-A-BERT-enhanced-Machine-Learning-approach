{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muajnstu/Multi-Class-Classification-of-YouTube-Videos-Using-A-BERT-enhanced-Machine-Learning-approach/blob/main/RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug\n",
        "!pip install ktrain\n",
        "!pip install tensorflow\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "w4uNfP_yjD_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/muajnstu/ML-Datasets/refs/heads/main/Youtube%20Video%20Dataset.csv')\n",
        "\n",
        "# Map category labels to integers\n",
        "df[\"Category\"] = df[\"Category\"].map({\n",
        "    \"travel blog\": 0,\n",
        "    \"Science&Technology\": 1,\n",
        "    \"Food\": 2,\n",
        "    \"Art&Music\": 3,\n",
        "    \"manufacturing\": 4,\n",
        "    \"History\": 5\n",
        "})\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Combine Title and Description into one text field\n",
        "df['processed_text'] = df['Title'] + \" \" + df['Description']\n",
        "\n",
        "# Drop unneeded columns\n",
        "df.drop(columns=['Title', 'Description', 'Videourl'], inplace=True)\n",
        "\n",
        "# Extract meaningful part using regex\n",
        "def extract_txt(text):\n",
        "    text = str(text)\n",
        "    match = re.search(r\"(?<=\\s\\-\\s).*\", text)\n",
        "    return match.group(0) if match else text\n",
        "\n",
        "df['processed_text'] = df['processed_text'].apply(extract_txt)\n",
        "\n",
        "# Basic text cleaning\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return text\n",
        "\n",
        "df['processed_text'] = df['processed_text'].apply(clean_text)\n",
        "\n",
        "# Rename for simplicity (optional)\n",
        "df.rename(columns={'processed_text': 'text'}, inplace=True)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    df['text'], df['Category'], test_size=0.2, random_state=42)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize texts\n",
        "def encode_texts(tokenizer, texts, max_length=256):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'][0])\n",
        "        attention_masks.append(encoded['attention_mask'][0])\n",
        "\n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_masks)\n",
        "\n",
        "# Tokenize each split\n",
        "train_input_ids, train_attention_masks = encode_texts(tokenizer, X_train)\n",
        "val_input_ids, val_attention_masks = encode_texts(tokenizer, X_val)\n",
        "test_input_ids, test_attention_masks = encode_texts(tokenizer, X_test)\n",
        "\n",
        "# Final label tensors\n",
        "y_train = tf.convert_to_tensor(y_train.values)\n",
        "y_val = tf.convert_to_tensor(y_val.values)\n",
        "y_test = tf.convert_to_tensor(y_test.values)\n",
        "\n",
        "\n",
        "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(class_names))\n",
        "\n",
        "# Model compilation\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Model training with validation data\n",
        "history = model.fit(\n",
        "    [train_input_ids, train_attention_masks],\n",
        "    y_train,\n",
        "    batch_size=20,\n",
        "    validation_data=([val_input_ids, val_attention_masks], y_val),\n",
        "    epochs=5,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate([test_input_ids, test_attention_masks], y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "FldXJxgfjZBa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}