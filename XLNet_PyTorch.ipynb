{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muajnstu/Multi-Class-Classification-of-YouTube-Videos-Using-A-BERT-enhanced-Machine-Learning-approach/blob/main/XLNet_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kebtBZQbM14",
        "outputId": "779cfc68-5bf1-4522-bd32-4e536000e0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VjcXTnvmm90",
        "outputId": "8983ad5f-4b7e-4dad-86aa-a8067bce5c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets scikit-learn --no"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YY79EnxmsIl",
        "outputId": "1f025c88-0b1f-42ab-b408-be425af6497c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "ambiguous option: --no (--no-binary, --no-build-isolation, --no-cache-dir, --no-clean, --no-color, --no-compile, --no-dependencies, --no-deps, --no-index, --no-input, --no-python-version-warning, --no-use-pep517, --no-user, --no-warn-conflicts, --no-warn-script-location?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    XLNetTokenizer,\n",
        "    XLNetForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# 1. Data Loading & Preprocessing\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/muajnstu/ML-Datasets/refs/heads/main/Youtube%20Video%20Dataset.csv')\n",
        "\n",
        "category_map = {\n",
        "    \"travel blog\": 0,\n",
        "    \"science&technology\": 1,\n",
        "    \"food\": 2,\n",
        "    \"art&music\": 3,\n",
        "    \"manufacturing\": 4,\n",
        "    \"history\": 5\n",
        "}\n",
        "df[\"Category\"] = df[\"Category\"].str.lower().str.strip().map(category_map)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df['processed_text'] = df['Title'].astype(str) + \" \" + df['Description'].astype(str)\n",
        "\n",
        "def extract_txt(text):\n",
        "    match = re.search(r\"(?<=\\s\\-\\s).*\", str(text))\n",
        "    return match.group(0) if match else text\n",
        "\n",
        "df['processed_text'] = df['processed_text'].apply(extract_txt)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['processed_text'] = df['processed_text'].apply(clean_text)\n",
        "df.rename(columns={'processed_text': 'text'}, inplace=True)\n",
        "\n",
        "# 2. Train/Val/Test Split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['Category'], test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 3. Tokenizer & Dataset\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "def encode_texts(tokenizer, texts, max_length=256):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = encode_texts(tokenizer, X_train)\n",
        "val_encodings = encode_texts(tokenizer, X_val)\n",
        "test_encodings = encode_texts(tokenizer, X_test)\n",
        "\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "val_labels = torch.tensor(y_val.values)\n",
        "test_labels = torch.tensor(y_test.values)\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = VideoDataset(train_encodings, train_labels)\n",
        "val_dataset = VideoDataset(val_encodings, val_labels)\n",
        "test_dataset = VideoDataset(test_encodings, test_labels)\n",
        "\n",
        "# 4. Model\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=6)\n",
        "\n",
        "# 5. TrainingArguments & Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_xlnet\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs_xlnet\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 6. Train & Evaluate\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Validation Results:\", eval_results)\n",
        "\n",
        "# 7. Test Set Evaluation\n",
        "test_preds = trainer.predict(test_dataset)\n",
        "test_preds_labels = test_preds.predictions.argmax(axis=1)\n",
        "print(\"Test Accuracy:\", accuracy_score(test_labels, test_preds_labels))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_labels, test_preds_labels))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(test_labels, test_preds_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "0lBPja6PaB2C",
        "outputId": "803170bd-0c23-4f9b-9254-88d8e4e4c4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoyajibnea\u001b[0m (\u001b[33mmoyajibnea-noakhali-science-and-technology-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250531_184115-o3kw5yjt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moyajibnea-noakhali-science-and-technology-university/huggingface/runs/o3kw5yjt' target=\"_blank\">./results_xlnet</a></strong> to <a href='https://wandb.ai/moyajibnea-noakhali-science-and-technology-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/moyajibnea-noakhali-science-and-technology-university/huggingface' target=\"_blank\">https://wandb.ai/moyajibnea-noakhali-science-and-technology-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/moyajibnea-noakhali-science-and-technology-university/huggingface/runs/o3kw5yjt' target=\"_blank\">https://wandb.ai/moyajibnea-noakhali-science-and-technology-university/huggingface/runs/o3kw5yjt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2805' max='2805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2805/2805 57:51, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.670900</td>\n",
              "      <td>0.500565</td>\n",
              "      <td>0.842997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.432400</td>\n",
              "      <td>0.481284</td>\n",
              "      <td>0.859946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.362000</td>\n",
              "      <td>0.487477</td>\n",
              "      <td>0.871543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.296400</td>\n",
              "      <td>0.486272</td>\n",
              "      <td>0.877788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.253100</td>\n",
              "      <td>0.548202</td>\n",
              "      <td>0.877788</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results: {'eval_loss': 0.4862721562385559, 'eval_accuracy': 0.8777876895628903, 'eval_runtime': 37.979, 'eval_samples_per_second': 29.516, 'eval_steps_per_second': 1.869, 'epoch': 5.0}\n",
            "Test Accuracy: 0.8707664884135472\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.82       204\n",
            "           1       0.78      0.94      0.85       220\n",
            "           2       0.91      0.86      0.88       191\n",
            "           3       0.87      0.89      0.88       173\n",
            "           4       0.92      0.88      0.90       162\n",
            "           5       0.95      0.87      0.91       172\n",
            "\n",
            "    accuracy                           0.87      1122\n",
            "   macro avg       0.88      0.87      0.87      1122\n",
            "weighted avg       0.88      0.87      0.87      1122\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[162  18   8   9   5   2]\n",
            " [  1 206   3   5   2   3]\n",
            " [  9   8 164   6   2   2]\n",
            " [  7   8   1 154   2   1]\n",
            " [  6  11   1   2 142   0]\n",
            " [  4  13   3   2   1 149]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}